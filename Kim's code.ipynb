{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ce6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "\n",
    "\n",
    "\n",
    "def load_images(img_dir, train_annot_dir, test_annot_dir, cls_dir):\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    os.chdir(img_dir)\n",
    "    if img_dir == 'additional_images/':\n",
    "        sc_data = np.array([cv2.imread(filename) for filename in glob.glob('*.jpg')])\n",
    "        data_labels = np.array([filename[:-4] for filename in glob.glob('*.jpg')])\n",
    "        \n",
    "        os.chdir('/home/ec2-user')\n",
    "        \n",
    "        return sc_data, data_labels\n",
    "    \n",
    "    else:\n",
    "        sc_data = np.array([cv2.imread(filename) for filename in sorted(glob.glob('*.jpg'), key = lambda x: int(os.path.splitext(x)[0]))])\n",
    "        \n",
    "        os.chdir('/home/ec2-user')\n",
    "    \n",
    "    if img_dir == 'saved_images/training/' or img_dir == 'saved_images/resized/training/':\n",
    "        meta = loadmat(cls_dir)\n",
    "        cls_name = meta['class_names'][0]\n",
    "        data_labels = np.array([img_label[0] for img_label in cls_name])\n",
    "\n",
    "        train_mat = loadmat(train_annot_dir)\n",
    "        train_annot = train_mat['annotations'][0]\n",
    "\n",
    "        train_data_class = np.array([img_annot[4][0][0] for img_annot in train_annot])\n",
    "        \n",
    "        print(\"Training image loading runtime:\", time.time()-start_ts)\n",
    "    \n",
    "        return sc_data, train_data_class, data_labels\n",
    "    \n",
    "    elif img_dir == 'saved_images/testing/' or img_dir == 'saved_images/resized/testing/':\n",
    "        test_mat = loadmat(test_annot_dir)\n",
    "        test_annot = test_mat['annotations'][0]\n",
    "\n",
    "        test_data_class = np.array([img_annot[4][0][0] for img_annot in test_annot])\n",
    "        \n",
    "        print(\"Testing image loading runtime:\", time.time()-start_ts)\n",
    "    \n",
    "        return sc_data, test_data_class\n",
    "    \n",
    "    elif img_dir == 'cars_train/cars_train/cars_train/':\n",
    "        meta = loadmat(cls_dir)\n",
    "        cls_name = meta['class_names'][0]\n",
    "        data_labels = np.array([img_label[0] for img_label in cls_name])\n",
    "\n",
    "        train_mat = loadmat(train_annot_dir)\n",
    "        train_annot = train_mat['annotations'][0]\n",
    "\n",
    "        train_data_class = np.array([img_annot[4][0][0] for img_annot in train_annot])\n",
    "        \n",
    "        x1 = np.array([img_annot[0][0][0] for img_annot in train_annot])\n",
    "        y1 = np.array([img_annot[1][0][0] for img_annot in train_annot])\n",
    "        x2 = np.array([img_annot[2][0][0] for img_annot in train_annot])\n",
    "        y2 = np.array([img_annot[3][0][0] for img_annot in train_annot])\n",
    "\n",
    "        bounding_boxes = []\n",
    "        for i in range(len(x1)):\n",
    "            bounding_boxes.append([x1[i], x2[i], y1[i], y2[i]])\n",
    "        bounding_boxes = np.array(bounding_boxes)\n",
    "\n",
    "        print(\"Training image loading runtime:\", time.time()-start_ts)\n",
    "\n",
    "        return sc_data, bounding_boxes, train_data_class, data_labels\n",
    "\n",
    "    elif img_dir == 'cars_test/':\n",
    "        test_mat = loadmat(test_annot_dir)\n",
    "        test_annot = test_mat['annotations'][0]\n",
    "\n",
    "        test_data_class = np.array([img_annot[4][0][0] for img_annot in test_annot])\n",
    "        \n",
    "        x1 = np.array([img_annot[0][0][0] for img_annot in test_annot])\n",
    "        y1 = np.array([img_annot[1][0][0] for img_annot in test_annot])\n",
    "        x2 = np.array([img_annot[2][0][0] for img_annot in test_annot])\n",
    "        y2 = np.array([img_annot[3][0][0] for img_annot in test_annot])\n",
    "\n",
    "        bounding_boxes = []\n",
    "        for i in range(len(x1)):\n",
    "            bounding_boxes.append([x1[i], x2[i], y1[i], y2[i]])\n",
    "        bounding_boxes = np.array(bounding_boxes)\n",
    "\n",
    "        print(\"Testing image loading runtime:\", time.time()-start_ts)\n",
    "\n",
    "        return sc_data, bounding_boxes, test_data_class\n",
    "\n",
    "\n",
    "\n",
    "def vehicle_detect(sc_data, bounding_boxes):\n",
    "    \n",
    "    detected_sc = []\n",
    "    for i in range(len(sc_data)):\n",
    "        x1 = bounding_boxes[i][0]\n",
    "        x2 = bounding_boxes[i][1]\n",
    "        y1 = bounding_boxes[i][2]\n",
    "        y2 = bounding_boxes[i][3]\n",
    "        detected_sc.append(sc_data[i][y1:y2, x1:x2])\n",
    "    \n",
    "    return detected_sc\n",
    "\n",
    "\n",
    "\n",
    "def save_as_jpg(train_img_data, test_img_data, save_dir = '/home/ec2-user'):\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    os.chdir(save_dir)\n",
    "    for i in range(len(train_img_data)):\n",
    "        img_name = 'training/%s.jpg' % (str(i))\n",
    "        cv2.imwrite(img_name, train_img_data[i])\n",
    "        \n",
    "    for i in range(len(test_img_data)):\n",
    "        img_name = 'testing/%s.jpg' % (str(i))\n",
    "        cv2.imwrite(img_name, test_img_data[i])\n",
    "\n",
    "    os.chdir('/home/ec2-user')\n",
    "    print(\"Image saving runtime:\", time.time()-start_ts)\n",
    "    \n",
    "    \n",
    "    \n",
    "def comb_data(train_data, test_data, train_data_class, test_data_class):\n",
    "    \n",
    "    # combining the training and testing dataset\n",
    "    all_data = np.concatenate((train_data, test_data), axis = 0)\n",
    "    all_class = np.concatenate((train_data_class, test_data_class), axis = 0)\n",
    "    \n",
    "    return all_data, all_class\n",
    "   \n",
    "   \n",
    "    \n",
    "def split_labels(input_label, wo_yr = 0, wo_mdyr = 0):\n",
    "    \n",
    "    make_List = []\n",
    "    model_List = []\n",
    "    year_List = []\n",
    "    type_List = []\n",
    "    \n",
    "    if wo_yr == 0 and wo_mdyr == 0:\n",
    "        for label in input_label:\n",
    "            split_label = label.split(' ')\n",
    "\n",
    "            if split_label[0] == 'Aston' or split_label[0] == 'Land' or split_label[0] == 'AM':\n",
    "                make_List.append(' '.join(split_label[0:2]))\n",
    "                model_List.append(' '.join(split_label[2:-2]))\n",
    "                type_List.append(split_label[-2])\n",
    "                year_List.append(split_label[-1])\n",
    "\n",
    "            else:\n",
    "                make_List.append(split_label[0])\n",
    "                model_List.append(' '.join(split_label[1:-2]))\n",
    "                type_List.append(split_label[-2])\n",
    "                year_List.append(split_label[-1])\n",
    "\n",
    "        return make_List, model_List, type_List, year_List\n",
    "    \n",
    "    elif wo_yr == 1:\n",
    "        for label in input_label:\n",
    "            split_label = label.split(' ')\n",
    "\n",
    "            if split_label[0] == 'Aston' or split_label[0] == 'Land' or split_label[0] == 'AM':\n",
    "                make_List.append(' '.join(split_label[0:2]))\n",
    "                model_List.append(' '.join(split_label[2:-1]))\n",
    "                type_List.append(split_label[-1])\n",
    "\n",
    "            else:\n",
    "                make_List.append(split_label[0])\n",
    "                model_List.append(' '.join(split_label[1:-1]))\n",
    "                type_List.append(split_label[-1])\n",
    "\n",
    "        return make_List, model_List, type_List\n",
    "    \n",
    "    elif wo_mdyr == 1:\n",
    "        for label in input_label:\n",
    "            split_label = label.split(' ')\n",
    "\n",
    "            if split_label[0] == 'Aston' or split_label[0] == 'Land' or split_label[0] == 'AM':\n",
    "                make_List.append(' '.join(split_label[0:2]))\n",
    "                type_List.append(split_label[-1])\n",
    "\n",
    "            else:\n",
    "                make_List.append(split_label[0])\n",
    "                type_List.append(split_label[-1])\n",
    "\n",
    "        return make_List, type_List\n",
    "\n",
    "\n",
    "\n",
    "def csv_convrt(data_class, make_List, model_List, type_List, year_List, csv_name, wo_yr = 0, wo_mdyr = 0):\n",
    "    \n",
    "    df_List = []\n",
    "    curr_List = []\n",
    "    \n",
    "    if wo_yr == 0 and wo_mdyr == 0:\n",
    "        for cls in data_class:\n",
    "            curr_List.append(cls)\n",
    "            curr_List.append(make_List[cls-1])\n",
    "            curr_List.append(model_List[cls-1])\n",
    "            curr_List.append(type_List[cls-1])\n",
    "            curr_List.append(year_List[cls-1])\n",
    "            df_List.append(curr_List)\n",
    "            curr_List = []\n",
    "        df_Array = np.array(df_List)\n",
    "\n",
    "        pd.DataFrame(df_Array).to_csv(csv_name, header = ['Class', 'Make', 'Model', 'Type', 'Year'], index = None)\n",
    "\n",
    "        return pd.DataFrame(df_Array, columns = ['Class', 'Make', 'Model', 'Type', 'Year'])\n",
    "        \n",
    "    elif wo_yr == 1:\n",
    "        for cls in data_class:\n",
    "            curr_List.append(cls)\n",
    "            curr_List.append(make_List[cls-1])\n",
    "            curr_List.append(model_List[cls-1])\n",
    "            curr_List.append(type_List[cls-1])\n",
    "            df_List.append(curr_List)\n",
    "            curr_List = []\n",
    "        df_Array = np.array(df_List)\n",
    "\n",
    "        pd.DataFrame(df_Array).to_csv(csv_name, header = ['Class', 'Make', 'Model', 'Type'], index = None)\n",
    "\n",
    "        return pd.DataFrame(df_Array, columns = ['Class', 'Make', 'Model', 'Type'])\n",
    "        \n",
    "    elif wo_mdyr == 1:\n",
    "        for cls in data_class:\n",
    "            curr_List.append(cls)\n",
    "            curr_List.append(make_List[cls-1])\n",
    "            curr_List.append(model_List[cls-1])\n",
    "            curr_List.append(type_List[cls-1])\n",
    "            curr_List.append(year_List[cls-1])\n",
    "            df_List.append(curr_List)\n",
    "            curr_List = []\n",
    "        df_Array = np.array(df_List)\n",
    "\n",
    "        pd.DataFrame(df_Array).to_csv(csv_name, header = ['Class', 'Make', 'Type'], index = None)\n",
    "\n",
    "        return pd.DataFrame(df_Array, columns = ['Class', 'Make', 'Type'])\n",
    "\n",
    "\n",
    "\n",
    "def rmv_year(data_labels, all_class):\n",
    "    \n",
    "    idx = np.unique(np.array([label[0:-5] for label in data_labels]), return_index = True)[1]\n",
    "    labels_wo_year = np.array([np.array([label[0:-5] for label in data_labels])[idx] for idx in sorted(idx)])\n",
    "    original_cls_cnt = max(all_class)\n",
    "\n",
    "    a = 0\n",
    "    b = 2\n",
    "    comb_List = []\n",
    "    for i in range(len(idx)-1):\n",
    "        comb_List.append(sorted(idx)[a:b])\n",
    "        a += 1\n",
    "        b += 1\n",
    "\n",
    "    for i in range(original_cls_cnt):\n",
    "        for j in range(len(comb_List)):\n",
    "            if comb_List[j][0] <= i < comb_List[j][1]:\n",
    "                class_idx = np.where(all_class == i+1)\n",
    "                all_class[class_idx] = comb_List.index(comb_List[j])+1\n",
    "                #print(i, comb_List.index(comb_List[j])+1)\n",
    "            elif j == len(comb_List)-1:\n",
    "                if i == comb_List[-1][1]:\n",
    "                    class_idx = np.where(all_class == i+1)\n",
    "                    all_class[class_idx] = comb_List.index(comb_List[j])+2\n",
    "                    #print(i, comb_List.index(comb_List[j])+2)\n",
    "        \n",
    "    return labels_wo_year, all_class\n",
    "\n",
    "\n",
    "\n",
    "def rmv_model_year(data_labels, all_class):\n",
    "    \n",
    "    labels_wo_make_year = []\n",
    "    for label in data_labels:\n",
    "        if label.split(' ')[0] == 'Aston' or label.split(' ')[0] == 'Land':\n",
    "            labels_wo_make_year.append(' '.join(label.split(' ')[0:2]))\n",
    "        else:\n",
    "            labels_wo_make_year.append(label.split(' ')[0])\n",
    "\n",
    "    idx = sorted(np.unique(labels_wo_make_year, return_index = True)[1])\n",
    "    labels_wo_make_year = np.array(labels_wo_make_year)[idx]\n",
    "    original_cls_cnt = max(all_class)\n",
    "\n",
    "    a = 0\n",
    "    b = 2\n",
    "    comb_List = []\n",
    "    for i in range(len(idx)-1):\n",
    "        comb_List.append(idx[a:b])\n",
    "        a += 1\n",
    "        b += 1\n",
    "\n",
    "    for i in range(original_cls_cnt):\n",
    "        for j in range(len(comb_List)):\n",
    "            if comb_List[j][0] <= i < comb_List[j][1]:\n",
    "                class_idx = np.where(all_class == i+1)\n",
    "                all_class[class_idx] = comb_List.index(comb_List[j])+1\n",
    "                #print(i, comb_List.index(comb_List[j])+1)\n",
    "            elif j == len(comb_List)-1:\n",
    "                if i == comb_List[-1][1]:\n",
    "                    class_idx = np.where(all_class == i+1)\n",
    "                    all_class[class_idx] = comb_List.index(comb_List[j])+2\n",
    "                    #print(i, comb_List.index(comb_List[j])+2)\n",
    "    \n",
    "    return labels_wo_make_year, all_class\n",
    "\n",
    "\n",
    "\n",
    "def gray_convrt(input_data):\n",
    "\n",
    "    # single rgb image input\n",
    "    if len(input_data.shape) == 3:\n",
    "        # converting input image to grayscale\n",
    "        data_gray = np.array(cv2.cvtColor(input_data, cv2.COLOR_BGR2GRAY))\n",
    "    \n",
    "    # multiple rgb image inputs\n",
    "    else:\n",
    "        # converting input images to grayscale\n",
    "        data_gray = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in input_data])\n",
    "\n",
    "    return data_gray\n",
    "\n",
    "\n",
    "\n",
    "def avg_size(x_data):\n",
    "    \n",
    "    x_size_List = []\n",
    "    y_size_List = []\n",
    "    aspect_ratio_List = []\n",
    "    for data in x_data:\n",
    "        # get the minimum pixel length (images are not squares)\n",
    "        x_size_List.append(data.shape[1])\n",
    "        y_size_List.append(data.shape[0])\n",
    "        aspect_ratio_List.append(data.shape[1]/data.shape[0])\n",
    "    \n",
    "    return (int(np.average(x_size_List)), int(np.average(y_size_List))), x_size_List, y_size_List, aspect_ratio_List\n",
    "\n",
    "\n",
    "\n",
    "def resize_all(input_data, size = (50, 50)):\n",
    "\n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # resizing input images\n",
    "    data_resized = np.array([cv2.resize(img, size) for img in input_data])\n",
    "\n",
    "    print(\"Data resizing runtime:\", time.time()-start_ts)\n",
    "    \n",
    "    return data_resized\n",
    "\n",
    "\n",
    "\n",
    "def clf_reshape(input_data):\n",
    "    \n",
    "    # image flattening, reshaping the data to the (samples, feature) matrix format\n",
    "    n_samples = len(input_data)\n",
    "    data_reshaped = input_data.reshape((n_samples, -1))\n",
    "    \n",
    "    return data_reshaped\n",
    "\n",
    "\n",
    "\n",
    "def canny_edge_convrt(input_data):\n",
    "\n",
    "    # single rgb image input\n",
    "    if len(input_data.shape) == 3:\n",
    "        data_edge = cv2.Canny(input_data, 100, 200)\n",
    "    \n",
    "    # multiple rgb image inputs\n",
    "    else:\n",
    "        # converting input images to edge detected images\n",
    "        data_edge = np.array([cv2.Canny(img, 100, 200) for img in input_data])\n",
    "\n",
    "    return data_edge\n",
    "\n",
    "\n",
    "    \n",
    "def hog_compute(input_data):\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    # single rgb image input\n",
    "    if len(input_data.shape) == 3:\n",
    "        # computing HOG features of input images\n",
    "        out, data_hog = hog(input_data, pixels_per_cell = (2, 2), visualize = True, multichannel = True)\n",
    "    \n",
    "    # multiple rgb image inputs\n",
    "    else:\n",
    "        # computing HOG features of input images\n",
    "        hog_output = [hog(img, pixels_per_cell = (2, 2), visualize = True, multichannel = True) for img in input_data]\n",
    "        data_hog = [hog_img for out, hog_img in hog_output]\n",
    "    \n",
    "    print(\"HOG feature computation runtime:\", time.time()-start_ts)\n",
    "\n",
    "    return data_hog\n",
    "\n",
    "\n",
    "\n",
    "def compare_thresh(input_data):\n",
    "    \n",
    "    # rgb image input\n",
    "    if len(input_data.shape) == 3:\n",
    "        img = gray_convrt(input_data)\n",
    "    \n",
    "    fig = plt.figure(figsize = (12, 30))\n",
    "    \n",
    "    # global thresholding\n",
    "    ret1, th1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Adaptive Mean Thresholding\n",
    "    th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Adaptive Gaussian Thresholding\n",
    "    th3 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Otsu's thresholding\n",
    "    ret4, th4 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Otsu's thresholding after Gaussian filtering\n",
    "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    ret5, th5 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    titles = ['Global Thresholding (v = 127)', 'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding', \\\n",
    "              \"Otsu's Thresholding\", \"Otsu's Thresholding w/ Gaussian Filtering\"]\n",
    "    images = [th1, th2, th3, th4, th5]\n",
    "    \n",
    "    # plotting 5 different thresholding methods\n",
    "    for i in range(5):\n",
    "        plt.subplot(5, 1, i+1)\n",
    "        plt.imshow(images[i], 'gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.xticks([]),plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def under_sample(x_data, y_data):\n",
    "\n",
    "    # obtaining class labels and their counts\n",
    "    labels_arr, class_count = np.unique(y_data, return_counts = True)\n",
    "\n",
    "    counter = 0\n",
    "    # for each class label, performing random under-sampling using the indices\n",
    "    for cls in labels_arr:\n",
    "        get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]\n",
    "        cls_idx = get_indexes(cls, y_data)\n",
    "        \n",
    "        # obtaining all image instances for the class label \"cls\"\n",
    "        reshaped_subset = x_data[cls_idx]\n",
    "        class_subset = y_data[cls_idx]\n",
    "        \n",
    "        # obtaining the indices for the purpose of random sampling without replacement\n",
    "        idx = np.random.choice(np.arange(len(reshaped_subset)), min(class_count), replace = False)\n",
    "        \n",
    "        if counter == 0:\n",
    "            # applying the randomly sampled indices on the subsets\n",
    "            x_und_smpl_data = reshaped_subset[idx]\n",
    "            y_und_smpl_data = class_subset[idx]\n",
    "            counter += 1\n",
    "        else:\n",
    "            # applying the randomly sampled indices on the subsets\n",
    "            x_und_smpl_data = np.concatenate((x_und_smpl_data, reshaped_subset[idx]))\n",
    "            y_und_smpl_data = np.concatenate((y_und_smpl_data, class_subset[idx]))\n",
    "            counter += 1\n",
    "\n",
    "    shuf_idx = np.random.permutation(len(x_und_smpl_data))\n",
    "    x_und_smpl_data, y_und_smpl_data = x_und_smpl_data[shuf_idx], y_und_smpl_data[shuf_idx]\n",
    "        \n",
    "    return x_und_smpl_data, y_und_smpl_data\n",
    "\n",
    "\n",
    "\n",
    "def test_to_train(number, train_path = '/home/ec2-user/stanford-car-dataset-by-classes-folder/car_data/car_data/train/',\\\n",
    "                  test_path = '/home/ec2-user/stanford-car-dataset-by-classes-folder/car_data/car_data/test/'):\n",
    "    \n",
    "    train_path = '/home/ec2-user/stanford-car-dataset-by-classes-folder/car_data/car_data/train/'\n",
    "    test_path = '/home/ec2-user/stanford-car-dataset-by-classes-folder/car_data/car_data/test/'\n",
    "    test_dir = os.listdir(test_path)\n",
    "\n",
    "    # redistributing images from the testing set to the training set to obtain rougly 70/30 split post under-sampling\n",
    "    for folder in test_dir:\n",
    "        folder_read = test_path + folder + '/*.jpg'\n",
    "\n",
    "        # reading from each class directory folder\n",
    "        filelist = glob.glob(folder_read)\n",
    "\n",
    "        # randomly choosing minimum individual class count\n",
    "        idx = np.random.choice(np.arange(len(filelist)), number, replace = False)\n",
    "        mv_filelist = np.array([file[-9:] for file in filelist])[idx]\n",
    "\n",
    "        # copying the selected images from the train folder to the und folder\n",
    "        for mv_file in mv_filelist:\n",
    "            src_dir = test_path + folder + '/' + mv_file\n",
    "            ult_dir = train_path + folder + '/' + mv_file\n",
    "            shutil.move(src_dir, ult_dir)\n",
    "\n",
    "\n",
    "\n",
    "def from_fold_und_smpl(train_path = '/home/ec2-user/stanford-car-dataset-by-classes-folder/car_data/car_data/train/',\\\n",
    "                       test_path = '/home/ec2-user/stanford-car-dataset-by-classes-folder/car_data/car_data/test/',\\\n",
    "                       und_path = '/home/ec2-user/stanford-car-dataset-by-classes-folder/car_data/car_data/und/'):\n",
    "    \n",
    "    train_dir = os.listdir(train_path)\n",
    "    test_dir = os.listdir(test_path)\n",
    "\n",
    "    # undersampling the training set\n",
    "    len_List = []\n",
    "    for folder in train_dir:\n",
    "        folder_read = train_path + folder + '/*.jpg'\n",
    "\n",
    "        # making the same directory forlders\n",
    "        os.mkdir(und_path + 'train/' + folder)\n",
    "        \n",
    "        # reading from each class directory folder\n",
    "        filelist = glob.glob(folder_read)\n",
    "        len_List.append(len(filelist))\n",
    "\n",
    "    for folder in train_dir:\n",
    "        folder_read = train_path + folder + '/*.jpg'\n",
    "        \n",
    "        # reading from each class directory folder\n",
    "        filelist = glob.glob(folder_read)\n",
    "        \n",
    "        # randomly choosing minimum individual class count\n",
    "        idx = np.random.choice(np.arange(len(filelist)), min(len_List), replace = False)\n",
    "        und_filelist = np.array([file[-9:] for file in filelist])[idx]\n",
    "        \n",
    "        # copying the selected images from the train folder to the und folder\n",
    "        for und_file in und_filelist:\n",
    "            src_dir = train_path + folder + '/' + und_file\n",
    "            ult_dir = und_path + 'train/' + folder + '/' + und_file\n",
    "            shutil.copy(src_dir, ult_dir)\n",
    "    \n",
    "    for folder in test_dir:\n",
    "        folder_read = test_path + folder + '/*.jpg'\n",
    "        \n",
    "        os.mkdir(und_path + 'test/' + folder)\n",
    "        \n",
    "        # reading from each class directory folder\n",
    "        filelist = glob.glob(folder_read)\n",
    "\n",
    "        for file in filelist:\n",
    "            src_dir = test_path + folder + '/' + file[-9:]\n",
    "            ult_dir = und_path + 'test/' + folder + '/' + file[-9:]\n",
    "            shutil.copy(src_dir, ult_dir)\n",
    "\n",
    "\n",
    "def from_fold_und_smpl_reset(und_path = '/home/ec2-user/stanford-car-dataset-by-classes-folder/car_data/car_data/und/'):\n",
    "\n",
    "    shutil.rmtree(und_path, ignore_errors = True)\n",
    "    os.mkdir(und_path)\n",
    "    os.mkdir(und_path + 'train')\n",
    "    os.mkdir(und_path + 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc0274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
